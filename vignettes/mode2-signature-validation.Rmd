---
title: "Mode 2: Signature Validation"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Mode 2: Signature Validation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
```

```{r setup}
library(endoSignatureR)
library(rsample)
```

# Mode 2: Signature Validation

Mode 2 (Signature Validation) enables training and validating new gene signatures on labeled cohorts using best practices including in-fold preprocessing, DE screening, nested cross-validation, LASSO with calibration, and optional stability selection. This workflow is designed for researchers who want to train their own signatures on new datasets and compare them to the pre-trained signature.

## Overview

Mode 2 provides a comprehensive pipeline for signature training that enforces anti-leakage practices:

1. **Data Preparation**: Load training dataset with balanced labels
2. **Cross-Validation Splits**: Create stratified CV splits with fixed seeds for reproducibility
3. **In-Fold Preprocessing**: Apply transforms/filters and gene selection within CV folds only
4. **Training Pipeline**: Nested CV with glmnet (LASSO/Elastic Net)
5. **Calibration**: Probability calibration (Platt/Isotonic)
6. **Stability Selection**: Resampling frequencies for feature selection
7. **Comparison**: Compare new signature to pre-trained signature

This vignette demonstrates Phase 2.1 functionality: **Data Preparation, Splits, and Anti-leakage Guards**. Future phases will add training, calibration, and comparison features.

## Data Loading

Mode 2 requires labeled data (counts + phenotype + optional annotation). Load your data files using `esr_loadFromFiles()`:

```{r mode2-data-loading, eval=FALSE}
# Load labeled data (required for Mode 2)
user_data <- esr_loadFromFiles(
  counts_file = "my_counts.tsv",
  pheno_file = "my_metadata.tsv",  # Required: contains sample_id and group
  annot_file = "my_annotation.tsv",  # Optional: for gene symbols
  validate = TRUE,  # Run validation checks
  align_ids = TRUE   # Align sample/gene IDs between files
)

# Check structure
str(user_data, max.level = 1)
# List with: counts, pheno, annot, issues (if validate=TRUE)

# Check validation issues
if (nrow(user_data$issues) > 0) {
  print(user_data$issues)
}
```

For detailed information on data loading functions, see the "Loading Your Own Data" section in the package introduction vignette: `vignette("endoSignatureR-intro", package = "endoSignatureR")`.

## Data Preparation

### Loading the Training Mini Dataset

The `gse201926_trainmini` dataset is a medium-sized subset (800-1500 genes × 12 samples) designed for fast nested CV in vignettes and tests (nested CV <60s). It maintains class balance (6 PS + 6 PIS) and is derived from the full GSE201926 dataset.

```{r load-data}
data(gse201926_trainmini)

# Examine structure
str(gse201926_trainmini)

# Check dimensions
dim(gse201926_trainmini$counts)

# View sample metadata
gse201926_trainmini$pheno

# Verify class balance
table(gse201926_trainmini$pheno$group)
```

### Dataset Characteristics

The training mini dataset:

- **Counts**: Raw read counts matrix (genes × samples)
- **Pheno**: Sample metadata with `sample_id`, `group` (PS/PIS), and `title`
- **Annot**: Gene annotation for selected genes
- **Class Balance**: Equal distribution of PS and PIS samples
- **Gene Selection**: Selected based on variance (preferring protein-coding)

## Cross-Validation Splits

### Creating CV Splits with Fixed Seeds

Cross-validation splits are essential for model training and validation. We use stratified K-fold CV to maintain class balance across folds. Fixed seeds ensure reproducibility.

```{r create-splits}
# Create outer CV splits with fixed seed
set.seed(12345)
outer_splits <- rsample::vfold_cv(
  data = gse201926_trainmini$pheno,
  v = 3,
  strata = "group",
  breaks = 2
)

# Examine splits
outer_splits
```

### Examining Split Structure

Each CV split contains training and testing data. We can extract these using `rsample::training()` and `rsample::testing()`.

```{r examine-splits}
# Get training/test data for first fold
train_data <- rsample::training(outer_splits$splits[[1]])
test_data <- rsample::testing(outer_splits$splits[[1]])

# Check class balance in first fold
cat("Training set:\n")
print(table(train_data$group))
cat("\nTest set:\n")
print(table(test_data$group))

# Verify all samples are covered
all_samples <- union(train_data$sample_id, test_data$sample_id)
cat("\nTotal samples covered:", length(all_samples), "\n")
```

### Using Pre-computed Demo Splits

For reproducible examples in vignettes and tests, we provide pre-computed splits in `folds_demo`. These splits are created with fixed seeds and stored for fast access.

```{r demo-folds}
data(folds_demo)

# Check structure
str(folds_demo)

# Access outer splits
folds_demo$outer_splits

# Access inner splits for first outer fold
if (!is.null(folds_demo$inner_splits[[1]])) {
  head(folds_demo$inner_splits[[1]])
}
```

## Anti-leakage Guards

### In-Fold Transformation and Filtering

A critical requirement for valid machine learning workflows is preventing data leakage. In-fold preprocessing ensures that preprocessing parameters are computed from training data only, then applied to test data.

The `esr_transformInFold()` function applies transforms (log1p-CPM) and filters (CPM threshold) within CV folds, computing parameters from training data only.

```{r transform-infold}
# Transform counts in-fold (anti-leakage)
result <- esr_transformInFold(
  split = outer_splits$splits[[1]],
  counts = gse201926_trainmini$counts,
  pheno = gse201926_trainmini$pheno,
  transform = "log1p-cpm",
  cpm_min = 1,
  cpm_min_samples = 4
)

# Check structure
str(result)

# Examine transformed matrices
cat("Training matrix dimensions:", dim(result$mat_t_train), "\n")
cat("Test matrix dimensions:", dim(result$mat_t_test), "\n")
cat("Genes kept:", length(result$genes_keep), "\n")

# Verify same genes in training and test
identical(colnames(result$mat_t_train), colnames(result$mat_t_test))
```

### In-Fold Gene Selection

Gene selection must also happen within CV folds to prevent leakage. The `esr_selectDEInFold()` function performs DE analysis on training data only, then selects top-K genes based on DE statistics.

```{r select-infold}
# First, transform full dataset for DE analysis
mat_t <- esr_transform_log1p_cpm(gse201926_trainmini$counts)

# Select top genes in-fold (DE method)
selected_de <- esr_selectDEInFold(
  split = outer_splits$splits[[1]],
  mat_t = mat_t,
  pheno = gse201926_trainmini$pheno,
  group_col = "group",
  n = 50,
  method = "de",
  seed = 123
)

cat("Selected genes (DE method):", length(selected_de), "\n")
head(selected_de)

# Select top genes in-fold (variance method)
selected_var <- esr_selectDEInFold(
  split = outer_splits$splits[[1]],
  mat_t = mat_t,
  pheno = gse201926_trainmini$pheno,
  n = 50,
  method = "variance",
  seed = 123
)

cat("\nSelected genes (variance method):", length(selected_var), "\n")
head(selected_var)
```

## Split Determinism

### Reproducibility with Fixed Seeds

Fixed seeds ensure that CV splits are reproducible across runs. This is essential for reproducible research and debugging.

```{r split-determinism}
# Create splits with same seed twice
set.seed(12345)
splits1 <- rsample::vfold_cv(
  data = gse201926_trainmini$pheno,
  v = 3,
  strata = "group",
  breaks = 2
)

set.seed(12345)
splits2 <- rsample::vfold_cv(
  data = gse201926_trainmini$pheno,
  v = 3,
  strata = "group",
  breaks = 2
)

# Verify splits are identical
train1 <- sort(rsample::training(splits1$splits[[1]])$sample_id)
train2 <- sort(rsample::training(splits2$splits[[1]])$sample_id)
identical(train1, train2)
```

### In-Fold Preprocessing Determinism

In-fold preprocessing also produces deterministic outputs with fixed seeds.

```{r preprocessing-determinism}
# Transform in-fold twice with same seed
set.seed(123)
result1 <- esr_transformInFold(
  outer_splits$splits[[1]],
  gse201926_trainmini$counts,
  gse201926_trainmini$pheno
)

set.seed(123)
result2 <- esr_transformInFold(
  outer_splits$splits[[1]],
  gse201926_trainmini$counts,
  gse201926_trainmini$pheno
)

# Verify results are identical
identical(result1$genes_keep, result2$genes_keep)
identical(result1$mat_t_train, result2$mat_t_train)
```

## Key Concepts

### Why Anti-leakage Matters

Data leakage occurs when information from test/validation data influences model training or preprocessing decisions. This leads to:

- **Overly optimistic performance estimates**: Models appear to perform better than they actually do
- **Poor generalization**: Models fail on truly unseen data
- **Invalid conclusions**: Research findings are not reliable

**In-fold preprocessing prevents leakage by**:

- Computing CPM filtering parameters from training data only
- Applying training-based parameters to test data
- Performing gene selection on training data only
- Never using test/validation data to inform preprocessing

### Nested Cross-Validation

Nested CV uses:

- **Outer splits**: For model selection and performance estimation
- **Inner splits**: For hyperparameter tuning within each outer fold

This ensures that test data in outer folds never influences hyperparameter tuning decisions.

## Core Training and Tuning

### Training with Nested Cross-Validation

The `esr_trainEndometrialSignature()` function implements nested CV with glmnet (LASSO) for training PS vs PIS signatures. It uses in-fold preprocessing and coefficient aggregation (Option 2) for stability with small n.

**Note**: With small datasets (n=12), glmnet may produce warnings about "fewer than 8 observations per class". These are expected and reflect the challenge of model training with very small sample sizes. The warnings are suppressed in vignette examples for clarity.

```{r training-example}
# Load required packages
library(glmnet)
library(rsample)

# Load training data and folds
data(gse201926_trainmini)
data(folds_demo)

# Train signature with optimized parameters for small n (better class separation)
# Suppress glmnet warnings about small sample size (expected with n=12)
set.seed(123)
result <- suppressWarnings(esr_trainEndometrialSignature(
  X = gse201926_trainmini$counts,
  pheno = gse201926_trainmini$pheno,
  top_k = 50,  # Smaller for faster vignette
  outer_folds = NULL,  # Use folds_demo
  lambda_rule = "min",  # More genes (better for small n)
  min_folds = 1,  # Less strict consensus (better for small n)
  seed = 123
))

# Examine results
str(result, max.level = 2)
```

### Understanding Training Parameters

Key parameters for training:

- **`top_k`**: Number of top genes to select via DE analysis (default: 300)
- **`lambda_rule`**: λ selection rule - `"1se"` (sparser, default) or `"min"` (more genes, better for small n)
- **`min_folds`**: Minimum folds that must select a gene for consensus (default: 2 of 3; use 1 for small n to allow genes selected in single folds)
- **`aggregation_method`**: Method for aggregating coefficients - `"mean"` (default) or `"median"`

**Note for small datasets (n < 20)**: Using `lambda_rule = "min"` and `min_folds = 1` often produces better class separation and more interpretable signatures, as it's less restrictive with sparse models.

```{r training-parameters}
# Compare lambda rules (using optimized min_folds=1 for better results with small n)
# Suppress warnings for cleaner output
set.seed(123)
result_1se <- suppressWarnings(esr_trainEndometrialSignature(
  X = gse201926_trainmini$counts,
  pheno = gse201926_trainmini$pheno,
  top_k = 50,
  lambda_rule = "1se",
  min_folds = 1,  # Less strict for small n
  outer_folds = NULL,
  seed = 123
))

set.seed(123)
result_min <- suppressWarnings(esr_trainEndometrialSignature(
  X = gse201926_trainmini$counts,
  pheno = gse201926_trainmini$pheno,
  top_k = 50,
  lambda_rule = "min",
  min_folds = 1,  # Less strict for small n
  outer_folds = NULL,
  seed = 123
))

# Compare signature sizes
cat("1se signature: ", length(result_1se$signature$panel), " genes\n")
cat("min signature: ", length(result_min$signature$panel), " genes\n")

# Note: Signatures may be empty with very sparse models and strict consensus requirements
# This is acceptable for small datasets - adjust min_folds or lambda_rule if needed
```

### Signature Interpretation

The trained signature contains:

- **Panel**: Consensus gene IDs (selected in ≥`min_folds` folds)
- **Coefficients**: Aggregated LASSO coefficients (mean or median across folds)
- **Intercept**: Aggregated logistic regression intercept
- **Selection frequency**: Number of folds that selected each gene

```{r signature-interpretation}
# Extract signature (using optimized result_min with better class separation)
signature <- result_min$signature

# Examine signature components
cat("Panel size: ", length(signature$panel), " genes\n")

if (length(signature$panel) > 0) {
  cat("Intercept: ", signature$intercept, "\n")

  # View top genes by absolute coefficient
  if (length(signature$coefficients) > 0) {
    coef_abs <- abs(signature$coefficients)
    coef_abs <- sort(coef_abs, decreasing = TRUE)
    cat("\nTop genes by absolute coefficient:\n")
    print(head(coef_abs, 10))

    # Check selection frequencies
    cat("\nSelection frequencies:\n")
    print(table(signature$selection_frequency))
  }
} else {
  cat("Signature is empty (no consensus genes found).\n")
  cat("This can happen with very sparse LASSO models (1se rule) and strict consensus requirements (min_folds=2).\n")
  cat("Try: lambda_rule='min' or min_folds=1 for more genes.\n")
}
```

### Coefficient Aggregation (Option 2)

For small n (n=12) and weak signals, coefficient aggregation (Option 2) provides:

- **Lower variance**: Averaging out fold-specific noise
- **Higher stability**: Requiring consensus across folds
- **Better generalization**: More robust than single-fold signatures

```{r aggregation-explanation}
# Compare aggregation methods (using optimized parameters)
# Suppress warnings for cleaner output
set.seed(123)
result_mean <- suppressWarnings(esr_trainEndometrialSignature(
  X = gse201926_trainmini$counts,
  pheno = gse201926_trainmini$pheno,
  top_k = 50,
  lambda_rule = "min",
  min_folds = 1,
  aggregation_method = "mean",
  outer_folds = NULL,
  seed = 123
))

set.seed(123)
result_median <- suppressWarnings(esr_trainEndometrialSignature(
  X = gse201926_trainmini$counts,
  pheno = gse201926_trainmini$pheno,
  top_k = 50,
  lambda_rule = "min",
  min_folds = 1,
  aggregation_method = "median",
  outer_folds = NULL,
  seed = 123
))

# Compare consensus genes
cat("Mean aggregation: ", length(result_mean$signature$panel), " consensus genes\n")
cat("Median aggregation: ", length(result_median$signature$panel), " consensus genes\n")

# Show selection frequencies
if (length(result_mean$signature$panel) > 0) {
  cat("\nSelection frequencies (mean aggregation):\n")
  print(table(result_mean$signature$selection_frequency))
} else {
  cat("\nNo consensus genes found with current parameters.\n")
  cat("Try reducing min_folds or using lambda_rule='min' for more genes.\n")
}
```

### Performance Metrics

Training returns performance metrics from outer CV:

- **AUC**: Area under ROC curve (aggregated across all outer folds)
- **Accuracy**: Classification accuracy (using threshold 0.5)
- **Predictions**: Per-sample predictions with probabilities and labels

```{r performance-metrics}
# Examine performance metrics (using optimized result_min with better class separation)
metrics <- result_min$metrics

# Check if predictions exist and have data
has_predictions <- !is.null(metrics$predictions) &&
                  is.data.frame(metrics$predictions) &&
                  nrow(metrics$predictions) > 0

if (!has_predictions) {
  cat("WARNING: No predictions available. This suggests the CV loop may have failed.\n")
  cat("Possible causes:\n")
  cat("  - Sample ID mismatches between folds_demo and counts/pheno data\n")
  cat("  - Errors in in-fold preprocessing (transform or gene selection)\n")
  cat("  - Insufficient data for model training\n")
  cat("\nTry re-running the training chunk to regenerate results.\n\n")
}

# Handle NA metrics gracefully (check length > 0 first to avoid length-zero errors)
if (length(metrics$auc) > 0 && !is.na(metrics$auc)) {
  cat("AUC: ", round(metrics$auc, 3), "\n")
} else {
  cat("AUC: NA")
  if (!has_predictions) {
    cat(" (no predictions available - CV loop may have failed)")
  } else {
    cat(" (insufficient class balance in predictions)")
  }
  cat("\n")
}

if (length(metrics$accuracy) > 0 && !is.na(metrics$accuracy)) {
  cat("Accuracy: ", round(metrics$accuracy, 3), "\n")
} else {
  cat("Accuracy: NA")
  if (!has_predictions) {
    cat(" (no predictions available - CV loop may have failed)")
  } else {
    cat(" (insufficient data for computation)")
  }
  cat("\n")
}

# View predictions (check existence and type first to avoid length-zero errors)
if (has_predictions) {
  cat("\nSample predictions:\n")
  print(head(metrics$predictions, 10))

  # Check prediction distribution
  cat("\nPrediction probabilities:\n")
  print(summary(metrics$predictions$prob))

  # Check class balance in predictions
  cat("\nClass distribution in predictions:\n")
  print(table(metrics$predictions$label))
}
```

### Understanding Nested CV Performance

Nested CV provides unbiased performance estimates by:

1. **Outer CV**: Evaluates model generalization on held-out test sets
2. **Inner CV**: Tunes hyperparameters (λ selection) within each outer fold
3. **Aggregation**: Combines predictions across all outer folds for overall metrics

This ensures that test data never influences hyperparameter tuning decisions.

## Phase 2.3: Probability Calibration and Stability Selection

Phase 2.3 adds probability calibration and stability selection to improve prediction reliability and feature robustness.

### Probability Calibration

Raw model probabilities from LASSO may be miscalibrated (predicted probabilities don't match observed frequencies). Calibration fixes this by adjusting probabilities to better match actual class frequencies.

```{r phase2.3-calibration, eval = TRUE}
# Train with Platt calibration (default for small n)
result_platt <- esr_trainEndometrialSignature(
  X = gse201926_trainmini$counts,
  pheno = gse201926_trainmini$pheno,
  top_k = 100,
  outer_folds = NULL,  # Use folds_demo
  calibration_method = "platt",
  seed = 123
)

# Check calibration metrics
result_platt$metrics$brier_score
result_platt$metrics$ece

# Compare raw vs calibrated probabilities
head(result_platt$metrics$predictions[, c("prob", "prob_calibrated", "label")])
```

**Calibration Methods**:

- **Platt Scaling** (default): Logistic regression on raw probabilities. Suitable for small n (n=12).
- **Isotonic Regression**: Non-parametric monotonic transformation. More flexible but requires more data.

```{r phase2.3-calibration-methods, eval = FALSE}
# Try isotonic regression (if you have more data)
result_isotonic <- esr_trainEndometrialSignature(
  X = gse201926_trainmini$counts,
  pheno = gse201926_trainmini$pheno,
  top_k = 100,
  outer_folds = NULL,
  calibration_method = "isotonic",
  seed = 123
)
```

**Calibration Metrics**:

- **Brier Score**: Mean squared error between predicted probabilities and binary outcomes (lower is better, 0 = perfect calibration).
- **Expected Calibration Error (ECE)**: Weighted average of absolute difference between predicted probabilities and observed frequencies within bins (lower is better, 0 = perfect calibration).

### Stability Selection

Stability selection identifies genes that are consistently selected across multiple bootstrap resamples, providing more robust feature selection than single LASSO fits.

```{r phase2.3-stability, eval = TRUE}
# Train with stability selection enabled
# For small n (n=12), stability selection is disabled by default
# Enable it explicitly with fewer resamples for demonstration
result_stability <- esr_trainEndometrialSignature(
  X = gse201926_trainmini$counts,
  pheno = gse201926_trainmini$pheno,
  top_k = 100,
  outer_folds = NULL,
  stability_selection = TRUE,
  stability_resamples = 20,  # Fewer resamples for small n
  seed = 123
)

# Check stability frequencies
head(result_stability$stability$bootstrap_frequency, 10)

# Check stable genes (frequency > 0.7 threshold)
length(result_stability$stability$stable_genes)
head(result_stability$stability$stable_genes, 10)
```

**Stability Selection Features**:

- **Bootstrap Resampling**: Additional resampling beyond outer CV to assess gene selection robustness.
- **Selection Frequencies**: Proportion of resamples where each gene is selected (non-zero coefficient).
- **Threshold Filtering**: Filter signature to only include genes with high selection frequency (e.g., > 0.7).

**Note**: For small n (n<20), stability selection is disabled by default because bootstrap resampling on very small datasets produces unstable frequency estimates with high variance. Enable it explicitly if needed, but use fewer resamples (e.g., 20-50) for computational efficiency.

### Combining Calibration and Stability

```{r phase2.3-combined, eval = TRUE}
# Train with both calibration and stability
result_combined <- esr_trainEndometrialSignature(
  X = gse201926_trainmini$counts,
  pheno = gse201926_trainmini$pheno,
  top_k = 100,
  outer_folds = NULL,
  calibration_method = "platt",
  stability_selection = TRUE,
  stability_resamples = 20,
  seed = 123
)

# Check all metrics
result_combined$metrics[c("auc", "accuracy", "brier_score", "ece")]
result_combined$calibration$method
length(result_combined$stability$stable_genes)
```

## Phase 2.4: Performance and Comparison Visualizations

Phase 2.4 adds visualization functions to help users understand and evaluate signature performance. The package provides ROC curves, Precision-Recall curves, calibration curves, and comparison plots.

### Performance Visualization Functions

Three functions visualize signature performance:

- **`plotEndometrialROC()`**: Receiver Operating Characteristic (ROC) curve showing TPR vs FPR
- **`plotEndometrialPR()`**: Precision-Recall (PR) curve showing precision vs recall (useful for imbalanced datasets)
- **`plotEndometrialCalibration()`**: Calibration curve showing how well predicted probabilities match observed frequencies

### ROC Curves

ROC curves visualize the trade-off between true positive rate (TPR) and false positive rate (FPR) across different probability thresholds. AUC (Area Under Curve) represents classification performance (AUC = 1.0 = perfect, AUC = 0.5 = random).

```{r phase2.4-roc, eval = TRUE}
# Use training result from Phase 2.3
result <- result_combined

# Plot ROC curve with raw probabilities
p_roc_raw <- plotEndometrialROC(result$metrics$predictions, use_calibrated = FALSE)
print(p_roc_raw)

# Plot ROC curve with calibrated probabilities
p_roc_cal <- plotEndometrialROC(result$metrics$predictions, use_calibrated = TRUE)
print(p_roc_cal)
```

ROC curves show how well the signature discriminates between PS and PIS samples. Higher AUC indicates better discrimination. Calibrated probabilities may have different AUC than raw probabilities, depending on calibration quality.

### Precision-Recall Curves

PR curves are especially useful for imbalanced datasets. They show precision vs recall at different probability thresholds. PR-AUC represents average precision across all recall values. The baseline (random classifier) equals the prevalence of the positive class.

```{r phase2.4-pr, eval = TRUE}
# Plot PR curve with raw probabilities
p_pr_raw <- plotEndometrialPR(result$metrics$predictions, use_calibrated = FALSE)
print(p_pr_raw)

# Plot PR curve with calibrated probabilities
p_pr_cal <- plotEndometrialPR(result$metrics$predictions, use_calibrated = TRUE)
print(p_pr_cal)
```

PR curves focus on precision (how many of predicted positives are actually positive) and recall (how many actual positives are captured). Higher PR-AUC indicates better performance, especially when the positive class is rare.

### Calibration Curves

Calibration curves assess probability calibration quality. A well-calibrated model has predicted probabilities that match observed frequencies (points close to the diagonal y = x). Brier Score and ECE measure calibration quality (lower is better).

```{r phase2.4-calibration, eval = TRUE}
# Plot calibration curve with raw probabilities
p_cal_raw <- plotEndometrialCalibration(result$metrics$predictions, use_calibrated = FALSE)
print(p_cal_raw)

# Plot calibration curve with calibrated probabilities
p_cal_cal <- plotEndometrialCalibration(result$metrics$predictions, use_calibrated = TRUE)
print(p_cal_cal)
```

Calibration curves show whether probabilities can be interpreted as actual risk estimates. If points are above the diagonal, the model overestimates probabilities; if below, it underestimates. Well-calibrated probabilities are essential for clinical decision-making.

### Comparison Visualization

The `plotEndometrialComparison()` function creates side-by-side comparison plots to compare pre-trained vs new signature performance. It generates ROC, PR, and calibration curves, plus a metrics comparison table.

```{r phase2.4-comparison, eval = TRUE}
# Generate comparison plots (new signature only for now)
comparison_plots <- plotEndometrialComparison(
  pretrained_result = NULL,  # Will compare with pre-trained in Phase 3
  new_result = result,
  metrics_to_plot = c("roc", "pr", "calibration"),
  show_metrics_table = TRUE
)

# Display ROC comparison
print(comparison_plots$roc)

# Display PR comparison
print(comparison_plots$pr)

# Display calibration comparison
print(comparison_plots$calibration)

# Display metrics table
comparison_plots$metrics_table
```

Comparison plots help researchers benchmark their new signatures against the pre-trained signature (when available in Phase 3). The metrics table compares AUC, accuracy, Brier Score, and ECE between signatures.

### Interpreting Performance Plots

**ROC Curves**:

- **AUC > 0.9**: Excellent discrimination
- **AUC 0.7-0.9**: Good discrimination
- **AUC 0.5-0.7**: Poor discrimination
- **AUC = 0.5**: Random (no discrimination)

**PR Curves**:

- **PR-AUC > baseline**: Better than random
- **PR-AUC close to baseline**: Poor performance on imbalanced data
- Higher PR-AUC indicates better precision-recall trade-off

**Calibration Curves**:

- **Points on diagonal**: Perfect calibration
- **Points above diagonal**: Model overestimates probabilities
- **Points below diagonal**: Model underestimates probabilities
- **Lower Brier Score/ECE**: Better calibration

### Performance Visualization Best Practices

1. **Always plot both raw and calibrated probabilities** to assess calibration impact
2. **Use ROC for balanced datasets** and **PR for imbalanced datasets**
3. **Check calibration curves** to ensure probabilities can be interpreted as risk estimates
4. **Compare signatures** using comparison plots when available
5. **Interpret metrics in context**: AUC, accuracy, Brier Score, and ECE together provide comprehensive performance assessment

## Phase 2.5: Exports and Model Card

Phase 2.5 adds export functionality to enable reproducible signature sharing and model transparency. The package provides three export formats: CSV (signature panel and coefficients), JSON (preprocessing recipe and training parameters), and Markdown (model card with provenance and performance).

### Export Functions

The `esr_exportSignature()` function exports trained signatures as reproducible artifacts:

- **endometrial_signature.csv**: Gene panel with coefficients, selection frequencies, and optional bootstrap frequencies
- **endometrial_recipe.json**: Preprocessing recipe, training parameters, signature metadata, and reproducibility information
- **endometrial_model_card.md**: Model documentation with provenance, performance metrics, limitations, and intended use

### Exporting Signatures

```{r phase2.5-export, eval = TRUE}
# Use training result from Phase 2.3 (result_combined contains calibration and stability)
# Export to temp directory
export_dir <- tempfile()
dir.create(export_dir)

# Export all formats (CSV, JSON, Markdown)
paths <- esr_exportSignature(
  signature = result_combined$signature,
  result = result_combined,
  dir = export_dir,
  formats = c("csv", "json", "md")
)

# Display exported file paths
paths
```

### Examining Exported Files

```{r phase2.5-examine-csv, eval = TRUE}
# Read CSV export
signature_csv <- readr::read_csv(
  file.path(export_dir, "endometrial_signature.csv"),
  show_col_types = FALSE
)

# Display first few rows
head(signature_csv, n = 10)
```

The CSV file contains:

- `gene_id`: Gene identifier
- `coefficient`: Aggregated coefficient value
- `selection_frequency`: Number of outer folds that selected this gene
- `bootstrap_frequency`: Bootstrap resampling frequency (if stability selection was performed)

```{r phase2.5-examine-json, eval = TRUE}
# Read JSON export
recipe_json <- jsonlite::fromJSON(
  file.path(export_dir, "endometrial_recipe.json")
)

# Display structure
str(recipe_json, max.level = 2)
```

The JSON file contains:

- `preprocessing`: Transformation method, CPM thresholds, top-K selection
- `training`: CV settings, lambda rule, aggregation method, calibration method
- `signature`: Number of genes, intercept, gene namespace
- `reproducibility`: Seed, package version, R version

```{r phase2.5-examine-model-card, eval = TRUE}
# Read model card
model_card <- readLines(file.path(export_dir, "endometrial_model_card.md"))

# Display first 30 lines
head(model_card, n = 30)
```

The model card includes:

- **Model Details**: Name, version, date, package and R versions
- **Model Purpose**: Description of signature classification task
- **Training Data**: Sample size, gene count, class balance
- **Model Architecture**: Algorithm, preprocessing, CV structure, aggregation method
- **Performance Metrics**: AUC, accuracy, Brier Score, ECE, signature size
- **Calibration**: Calibration method and parameters
- **Stability Selection**: Bootstrap resamples, threshold, stable genes (if computed)
- **Limitations**: Tissue specificity, small n uncertainty, binary only, batch effects
- **Intended Use**: Use case, target audience, not intended for
- **Training Parameters**: Complete training configuration
- **Reproducibility**: Seeds, CV folds, package versions
- **References**: Citations for methods

### Export Options

```{r phase2.5-export-options, eval = TRUE}
# Export only CSV
esr_exportSignature(
  signature = result_combined$signature,
  result = result_combined,
  dir = tempfile(),
  formats = "csv"
)

# Export only JSON
esr_exportSignature(
  signature = result_combined$signature,
  result = result_combined,
  dir = tempfile(),
  formats = "json"
)

# Export without intercept in CSV
esr_exportSignature(
  signature = result_combined$signature,
  result = result_combined,
  dir = tempfile(),
  formats = "csv",
  include_intercept = FALSE
)
```

### Model Card Interpretation

**Purpose**: The model card documents signature provenance, performance, and limitations to enable informed use.

**Key Sections**:

1. **Performance Metrics**: Assess signature quality (AUC, accuracy, calibration)
2. **Limitations**: Understand tissue specificity, small n uncertainty, and constraints
3. **Intended Use**: Determine appropriate use cases and target audience
4. **Training Parameters**: Reproduce training configuration
5. **Reproducibility**: Track seeds, folds, and package versions for reproducibility

**Best Practices**:

1. **Always include model card** when sharing signatures to ensure transparency
2. **Review limitations** before applying signature to new datasets
3. **Check intended use** to ensure signature matches use case
4. **Verify reproducibility** using seeds and package versions
5. **Document changes** if signature is modified or retrained

### Using Exported Artifacts

Exported artifacts enable:

1. **Reproducibility**: Reconstruct signature and preprocessing recipe from exported files
2. **Sharing**: Share signatures with collaborators without requiring full training results
3. **Version Control**: Track signature versions via exported files in Git or artifact repositories
4. **Portability**: Apply signatures to new data using exported coefficients and recipe
5. **Validation**: Independent validation using exported artifacts

### Export Best Practices

1. **Always export with full result** (include `result` parameter) to ensure complete metadata
2. **Include all formats** (CSV, JSON, Markdown) for maximum portability
3. **Store exports with version information** (e.g., timestamps, Git tags)
4. **Document export context** (e.g., training dataset, date, purpose)
5. **Verify exported files** can be read back correctly before sharing

## Next Steps

Phase 2.5 completes Phase 2 (Mode 2 Pipeline). Future phases will add:

- **Phase 3**: Pre-trained signature loading and scoring (Mode 1)
- **Phase 4**: Plots and reporting enhancements
- **Phase 5**: Shiny app integration

## Summary

Mode 2 (Signature Validation) provides a comprehensive pipeline for training and validating gene signatures. Phases 2.1-2.5 establish:

1. **Data Preparation**: `gse201926_trainmini` provides balanced training data for fast nested CV
2. **CV Splits**: Stratified splits with fixed seeds for reproducibility (`folds_demo`)
3. **Anti-leakage**: In-fold preprocessing prevents data leakage (`esr_transformInFold()`, `esr_selectDEInFold()`)
4. **Core Training**: Nested CV with glmnet (LASSO) for signature training
5. **Coefficient Aggregation**: Option 2 aggregates coefficients across folds for stability (small n)
6. **Probability Calibration**: Platt/Isotonic scaling for reliable predictions
7. **Stability Selection**: Bootstrap resampling for robust feature selection
8. **Performance Metrics**: AUC, accuracy, Brier Score, ECE, and predictions from outer CV
9. **Performance Visualization**: ROC, PR, and calibration curves for performance assessment
10. **Comparison Visualization**: Side-by-side comparison of pre-trained vs new signatures (when available)
11. **Signature Export**: CSV, JSON, and Markdown exports for reproducible sharing
12. **Model Cards**: Transparent documentation of signature provenance, performance, and limitations
13. **Determinism**: Fixed seeds ensure reproducible results

Phases 2.1-2.5 establish core training, calibration, stability, visualization, and export functionality that maintains best practices for small-sample ML while enabling reproducible signature sharing and model transparency.

## Session Info

```{r sessionInfo}
sessionInfo()
```

## References

This vignette uses the following packages and methods:

- Cross-validation: @rsample2025 for general resampling infrastructure
- Differential expression: @limma2015 for differential expression analyses
- LASSO regularization: @lasso2015 for behavior of LASSO with small sample sizes
- Code assistance: @copilot2025 for AI-powered code completions in this vignette

# [END]
